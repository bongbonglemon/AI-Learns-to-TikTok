{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: AI Learns to TikTok\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our model. \n",
    "\n",
    "Use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Get dataloader\n",
    "- [Step 2](#step2): Visualise poses\n",
    "- [Step 3](#step3): Set up models\n",
    "- [Step 4](#step4): Train models\n",
    "- [Step 5](#step5): (TODO) Validate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from data_loader import get_loader\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 1         # batch size\n",
    "input_size = 50           # dimensionality of image embedding and pose vecotr\n",
    "hidden_size = 500          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 5             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Visualise poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad frames: 8\n"
     ]
    }
   ],
   "source": [
    "# Obtain the batch.\n",
    "spectrogram, pose_coordinates = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 225, 50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change second index for different frame\n",
    "pose = pose_coordinates[0][45] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 50 to 25x2\n",
    "pose = pose.view(25,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pose[:,0],pose[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OH SHIT ITS UPSIDE DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(input_size)\n",
    "decoder = DecoderRNN(input_size, hidden_size, num_layers=2)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.L1Loss().cuda() if torch.cuda.is_available() else nn.L1Loss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(data_loader.dataset.num_vids / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights.\n",
    "encoder_file = \"encoder-5.pkl\"\n",
    "decoder_file = \"decoder-5.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [1/12], Loss: 40.4710\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [2/12], Loss: 42.0725\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [3/12], Loss: 39.0545\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [4/12], Loss: 31.7096\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [5/12], Loss: 30.2561\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [6/12], Loss: 43.0126\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [7/12], Loss: 42.8514\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [8/12], Loss: 26.0839\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [9/12], Loss: 32.0967\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [10/12], Loss: 34.9016\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [11/12], Loss: 36.5087\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/5], Step [12/12], Loss: 36.4031\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [1/12], Loss: 25.6834\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [2/12], Loss: 33.8202\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [3/12], Loss: 42.1318\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [4/12], Loss: 23.2841\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [5/12], Loss: 26.8307\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [6/12], Loss: 23.6304\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [7/12], Loss: 32.9212\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [8/12], Loss: 24.1615\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [9/12], Loss: 33.2481\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [10/12], Loss: 33.2196\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [11/12], Loss: 32.4638\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/5], Step [12/12], Loss: 28.7536\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [1/12], Loss: 31.3328\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [2/12], Loss: 31.1932\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [3/12], Loss: 35.3865\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [4/12], Loss: 26.2006\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [5/12], Loss: 33.5406\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [6/12], Loss: 26.2391\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [7/12], Loss: 27.9217\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [8/12], Loss: 33.4284\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [9/12], Loss: 21.3125\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [10/12], Loss: 25.0499\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [11/12], Loss: 34.5864\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/5], Step [12/12], Loss: 40.3629\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [1/12], Loss: 30.4240\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [2/12], Loss: 33.2091\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [3/12], Loss: 23.7080\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [4/12], Loss: 30.3670\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [5/12], Loss: 30.3513\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [6/12], Loss: 21.7007\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [7/12], Loss: 32.2374\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [8/12], Loss: 28.1873\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [9/12], Loss: 24.6143\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [10/12], Loss: 33.0483\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [11/12], Loss: 28.2464\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/5], Step [12/12], Loss: 25.9468\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [1/12], Loss: 30.4533\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [2/12], Loss: 28.2092\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [3/12], Loss: 28.1636\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [4/12], Loss: 21.6139\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [5/12], Loss: 27.9955\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [6/12], Loss: 32.5402\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [7/12], Loss: 34.3949\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [8/12], Loss: 34.3500\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [9/12], Loss: 21.3149\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [10/12], Loss: 24.8293\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [11/12], Loss: 34.2178\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/5], Step [12/12], Loss: 23.8393\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        spectrograms, pose_coordinates = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        pose_coordinates = pose_coordinates.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(spectrograms)\n",
    "        outputs = decoder(features, pose_coordinates.float()) #1x225x50\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs, pose_coordinates)\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item())\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "    \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))       \n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: (TODO) Validate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
