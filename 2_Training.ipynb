{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: AI Learns to TikTok\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we will train our model. \n",
    "\n",
    "Use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Get dataloader\n",
    "- [Step 2](#step2): Visualise poses\n",
    "- [Step 3](#step3): Set up models\n",
    "- [Step 4](#step4): Train models\n",
    "- [Step 5](#step5): (TODO) Validate models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Get dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from data_loader import get_loader\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 1         # batch size\n",
    "input_size = 50           # dimensionality of image embedding and pose vecotr\n",
    "hidden_size = 500          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 10             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Visualise poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad frames: 6\n"
     ]
    }
   ],
   "source": [
    "# Obtain the batch.\n",
    "spectrogram, pose_coordinates = next(iter(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 225, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pose_coordinates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change second index for different frame\n",
    "pose = pose_coordinates[0][45] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from 50 to 25x2\n",
    "pose = pose.view(25,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pose[:,0],pose[:,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OH SHIT ITS UPSIDE DOWN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: Set up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import EncoderCNN, DecoderRNN\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(input_size)\n",
    "decoder = DecoderRNN(input_size, hidden_size, num_layers=2)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.L1Loss().cuda() if torch.cuda.is_available() else nn.L1Loss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.0001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(data_loader.dataset.num_vids / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step4'></a>\n",
    "## Step 4: Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_every = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights.\n",
    "encoder_file = \"encoder-10.pkl\"\n",
    "decoder_file = \"decoder-10.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [1/12], Loss: 32.2552\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [2/12], Loss: 22.5850\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [3/12], Loss: 30.0769\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [4/12], Loss: 20.5242\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [5/12], Loss: 30.0028\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [6/12], Loss: 25.3800\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [7/12], Loss: 33.2055\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [8/12], Loss: 20.5503\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [9/12], Loss: 33.1252\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [10/12], Loss: 21.2814\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [11/12], Loss: 24.2213\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [1/10], Step [12/12], Loss: 31.9216\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [1/12], Loss: 27.6745\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [2/12], Loss: 31.7732\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [3/12], Loss: 24.3526\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [4/12], Loss: 32.8199\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [5/12], Loss: 39.4161\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [6/12], Loss: 33.3004\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [7/12], Loss: 39.2750\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [8/12], Loss: 25.5825\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [9/12], Loss: 22.9940\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [10/12], Loss: 20.7498\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [11/12], Loss: 28.3237\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [2/10], Step [12/12], Loss: 33.5041\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [1/12], Loss: 32.4507\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [2/12], Loss: 24.0677\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [3/12], Loss: 30.1553\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [4/12], Loss: 32.9910\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [5/12], Loss: 23.0144\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [6/12], Loss: 29.8991\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [7/12], Loss: 21.2368\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [8/12], Loss: 33.3837\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [9/12], Loss: 31.4338\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [10/12], Loss: 32.8807\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [11/12], Loss: 27.9991\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [3/10], Step [12/12], Loss: 23.9908\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [1/12], Loss: 32.4394\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [2/12], Loss: 25.4132\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [3/12], Loss: 24.0910\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [4/12], Loss: 32.4366\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [5/12], Loss: 22.5377\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [6/12], Loss: 32.0497\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [7/12], Loss: 30.1779\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [8/12], Loss: 32.5623\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [9/12], Loss: 30.0706\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [10/12], Loss: 20.4363\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [11/12], Loss: 31.9993\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [4/10], Step [12/12], Loss: 20.3357\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [1/12], Loss: 24.2543\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [2/12], Loss: 20.1481\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [3/12], Loss: 40.0494\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [4/12], Loss: 40.0753\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [5/12], Loss: 21.5973\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [6/12], Loss: 22.4578\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [7/12], Loss: 32.7203\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [8/12], Loss: 33.4332\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [9/12], Loss: 21.0737\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [10/12], Loss: 32.2800\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [11/12], Loss: 22.9207\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [5/10], Step [12/12], Loss: 20.8507\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [1/12], Loss: 20.6976\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [2/12], Loss: 39.7292\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [3/12], Loss: 24.2033\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [4/12], Loss: 32.3892\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [5/12], Loss: 30.0286\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [6/12], Loss: 25.4756\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [7/12], Loss: 20.3011\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [8/12], Loss: 22.4987\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [9/12], Loss: 20.2493\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [10/12], Loss: 21.6250\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [11/12], Loss: 28.1196\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [6/10], Step [12/12], Loss: 20.1752\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [1/12], Loss: 24.0951\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [2/12], Loss: 20.1624\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [3/12], Loss: 25.2893\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [4/12], Loss: 24.1534\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [5/12], Loss: 27.8292\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [6/12], Loss: 27.7650\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [7/12], Loss: 21.3191\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [8/12], Loss: 32.4388\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [9/12], Loss: 23.0409\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [10/12], Loss: 25.6552\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [11/12], Loss: 39.7487\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [7/10], Step [12/12], Loss: 32.2226\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [1/12], Loss: 21.1417\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [2/12], Loss: 33.4547\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [3/12], Loss: 39.5486\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [4/12], Loss: 20.5305\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [5/12], Loss: 29.9242\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [6/12], Loss: 29.9578\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [7/12], Loss: 39.4529\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [8/12], Loss: 22.5437\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [9/12], Loss: 32.1796\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [10/12], Loss: 25.3682\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [11/12], Loss: 32.3624\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [8/10], Step [12/12], Loss: 32.2739\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [1/12], Loss: 22.5011\n",
      "Number of bad frames: 6\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [2/12], Loss: 23.8952\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [3/12], Loss: 31.6454\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [4/12], Loss: 21.2301\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [5/12], Loss: 29.7356\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [6/12], Loss: 26.1372\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [7/12], Loss: 39.1350\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [8/12], Loss: 29.7840\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [9/12], Loss: 32.8253\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [10/12], Loss: 39.1073\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [11/12], Loss: 30.9141\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [9/10], Step [12/12], Loss: 33.9902\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [1/12], Loss: 21.7020\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [2/12], Loss: 28.3468\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [3/12], Loss: 38.9165\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [4/12], Loss: 28.4532\n",
      "Number of bad frames: 0\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [5/12], Loss: 22.7857\n",
      "Number of bad frames: 1\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [6/12], Loss: 30.2316\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [7/12], Loss: 33.3616\n",
      "Number of bad frames: 4\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [8/12], Loss: 25.3478\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [9/12], Loss: 33.1648\n",
      "Number of bad frames: 2\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [10/12], Loss: 21.7514\n",
      "Number of bad frames: 8\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [11/12], Loss: 32.8810\n",
      "Number of bad frames: 3\n",
      "Features.shape: torch.Size([1, 50])\n",
      "Poses.shape: torch.Size([1, 225, 50])\n",
      "Concat Inputs.shape: torch.Size([1, 225, 50])\n",
      "Output from LSTM.shape: torch.Size([1, 225, 500])\n",
      "Output from Linear.shape: torch.Size([1, 225, 50])\n",
      "Epoch [10/10], Step [12/12], Loss: 27.8962\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        spectrograms, pose_coordinates = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        spectrograms = spectrograms.to(device)\n",
    "        pose_coordinates = pose_coordinates.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(spectrograms)\n",
    "        outputs = decoder(features, pose_coordinates.float()) #1x225x50\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs, pose_coordinates)\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f' % (epoch, num_epochs, i_step, total_step, loss.item())\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats)\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "    \n",
    "        # Save the weights.\n",
    "        if i_step % save_every == 0:\n",
    "            torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "            torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))       \n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step5'></a>\n",
    "## Step 5: (TODO) Validate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
